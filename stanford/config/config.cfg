#***************************************************************
# High level stuff
[DEFAULT]
save_dir = SAVE_DIR
data_dir = DATA_DIR
lc = tmp
treebank = tmp
lang = tmp

[Configurable]
train_files = %(data_dir)s/%(lc)s-ud-train.conllu
parse_files = %(data_dir)s/%(lc)s-ud-dev.conllu
test_files = %(data_dir)s/%(lc)s-ud-test.conllu
verbose = True
name = None


#***************************************************************
# Vocab data structures
[Base Vocab]
cased = None
embed_size = 100

[Pretrained Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>
skip_header = True
name = pretrained
filename = EMBEDDINGS.VEC
cased = False
max_rank = 300000

[Tag Rep Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>
cased = False
name = tagrep
embed_keep_prob = .67

[Token Vocab]
name = tokens
embed_keep_prob = 0.5
min_occur_count = 2
max_rank = 300000

[Index Vocab]
special_tokens = <PAD>:<ROOT>

[Dep Vocab]
name = deps

[Head Vocab]
name = heads

[Word Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>
name = words
filename = %(save_dir)s/%(name)s.txt
cased = False

[Lemma Vocab]
name = lemmas
filename = %(save_dir)s/%(name)s.txt

[Tag Vocab]
special_tokens = PAD:ROOT:DROP:UNK
name = tags
filename = %(save_dir)s/%(name)s.txt
cased = True
embed_size = 100

[X Tag Vocab]
name = xtags
filename = %(save_dir)s/%(name)s.txt
embed_size = 100

[Feat Vocab]
name = feats
filename = %(save_dir)s/%(name)s.txt
embed_size = 100

[Rel Vocab]
special_tokens = pad:root:drop:unk
name = rels
filename = %(save_dir)s/%(name)s.txt
cased = True

[Subtoken Vocab]
max_rank = 0
n_buckets = 4
embed_model = CNNEmbed
embed_keep_prob = 1

[Char Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>:<START>:<STOP>
name = chars
filename = %(save_dir)s/%(name)s.txt
embed_model = RNNEmbed

[Subpos Vocab]
max_rank = 0
min_occur_count = 1
embed_model = CNNEmbed
embed_keep_prob = 1
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>
name = sub
filename = %(save_dir)s/%(name)s.txt
embed_size = 100

[Ngram Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>
name = ngrams
filename = %(save_dir)s/%(name)s.txt
embed_model = MLPEmbed

[Ngram Multivocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>
name = multi-ngram
max_n = 5
embed_model = MLPEmbed

[Bytepair Vocab]
name = bytepairs
filename = %(save_dir)s/%(name)s.txt
n_bytepairs = 500
embed_model = MLPEmbed

[Multivocab]
embed_keep_prob = 0.9
merge_strategy = add

[NN]
recur_cell = LSTMCell
n_layers = 3
mlp_func = leaky_relu
conv_func = leaky_relu
recur_size = 300
window_size = 5
mlp_size = 120
rnn_func = birnn
conv_keep_prob = .67
mlp_keep_prob = 0.67
recur_keep_prob = 0.67
ff_keep_prob = .67

[Base Cell]
forget_bias = 0
recur_func = tanh
recur_size = 300

[RNN Cell]
recur_func = leaky_relu
recur_size = 400

[RNN Embed]
rnn_func = rnn

[Base Tagger]
input_vocabs = words
output_vocabs = tags

[Base X Tagger]
input_vocabs = words
output_vocabs = tags:xtags

[Base UXF Tagger]
input_vocabs = chars:words
output_vocabs = tags:subxtags:subfeats

[Tagger]
name = tagger
n_layers = 2
recur_keep_prob = .5

[X Tagger]
name = xtagger
n_layers = 2
recur_keep_prob = .5

[UXF Tagger]
name = uxftagger
n_layers = 2
recur_keep_prob = 0.5

[Base Parser]
input_vocabs = chars:words:tags:subxtags:subfeats
output_vocabs = rels:heads

[Parser]
name = parser
arc_mlp_size = 500
rel_mlp_size = 170

[Multibucket]
n_buckets = 2
name = multibucket

[Bucket]
name = None

[Dataset]

[Trainset]
name = trainset
data_files = train_files
n_buckets = 10
batch_by = tokens
batch_size = 5000

[Parseset]
name = parseset
data_files = parse_files
n_buckets = 10
batch_by = tokens
batch_size = 50000

[Network]
name = network
subtoken_vocab = CharVocab
subpos_vocab = SubposVocab
nlp_model = Parser
min_train_iters = 100
max_train_iters = 30000
max_train_time = 40
validate_every = 120
min_percent_per_hour = 0.1
save_every = 1
quit_after_n_iters_without_improvement = 5000
per_process_gpu_memory_fraction = 0.15

[Radam Optimizer]
name = radam
learning_rate = 0.004
decay = 0.7
decay_steps = 5000
clip = 6
mu = 0.8
nu = 0.9
gamma = 0
chi = 0
epsilon = 1e-12

[Zipf]
n_zipfs = 3
name = zipf
filename = %(save_dir)s/%(name)s.txt
batch_size = 500
max_train_iters = 5000
print_every = 500

[Bucketer]
name = bucketer
filename = %(save_dir)s/%(name)s.txt

